
# coding: utf-8

# <h1> Segmenting and Clustering Neighborhoods in Toronto </h1>
# 
# <h2> IBM Data Science Professional Certificate </h2>
# 
# <h3> Capstone Week 3 Part 1 </h3>

# In[1]:


import pandas as pd
website_url = 'https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M'
table =pd.read_html(website_url)
table=table[0]
table.head()


# In[2]:


table.columns = table.iloc[0]
table = table.reindex(table.index.drop(0))
table.head()


# In[3]:


import numpy as np

table['Borough'].replace('Not assigned',np.nan,inplace=True)
table.head()


# In[4]:


# simply drop whole row with NaN in "Borough" column
table.dropna(subset=["Borough"], axis=0, inplace=True)

# reset index, because we droped rows
table.reset_index(drop=True, inplace=True)

table.head()


# <h4> Combine Postcode rows </h4>
# 
# More than one neighborhood can exist in one postal code area. For example, in the table on the Wikipedia page, you will notice that M5A is listed twice and has two neighborhoods: Harbourfront and Regent Park. These two rows will be combined into one row with the neighborhoods separated with a comma as shown in row 11 in the above table.

# In[5]:


table2= table.groupby(by=['Postcode','Borough']).agg(lambda x: ','.join(x))

table2.reset_index(level=['Postcode','Borough'], inplace=True)
table2.head()


# <h4> Fill not assigned neighborhood with borough </h4>
# 
# If a cell has a borough but a Not assigned neighborhood, then the neighborhood will be the same as the borough. So for the 9th cell in the table on the Wikipedia page, the value of the Borough and the Neighborhood columns will be Queen's Park.

# In[6]:


table2['Neighbourhood'].replace('Not assigned',table2['Borough'],inplace=True)

# test if the code above works
table2.loc[table2['Postcode']=='M7A',]


# <h4> Number of rows of my dataframe </h4>
# 
# In the last cell of your notebook, use the .shape method to print the number of rows of your dataframe.

# In[7]:


print('My datafram has {} rows'.format(table2.shape[0]))


# <h1> Capstone Week 3 Part 2 </h1>
# 
# Get the latitude and the longitude coordinates of each neighborhood.

# In[8]:


get_ipython().system(' pip install geocoder')


# In[9]:


import geocoder # import geocoder

# initialize your variable to None
lat_lng_coords = None

postal_code = 'M5A'

# loop until you get the coordinates
while(lat_lng_coords is None):
  g = geocoder.arcgis('{}, Toronto, Ontario'.format(postal_code))
  lat_lng_coords = g.latlng

latitude = lat_lng_coords[0]
longitude = lat_lng_coords[1]


# In[10]:


print(latitude)
print(longitude)


# In[11]:


coordinates= pd.read_csv('Geospatial_Coordinates.csv')
coordinates.head()


# In[12]:


table3=pd.merge(table2,coordinates,left_on='Postcode', right_on='Postal Code')
table3.drop(columns='Postal Code', inplace=True)
table3.head()


# In[13]:


table3.shape


# <h1> Capstone Week 3 Part 3 </h1>
# 
# Explore and cluster the neighborhoods in Toronto. You can decide to work with only boroughs that contain the word Toronto and then replicate the same analysis we did to the New York City data. It is up to you.
# 
# Just make sure:
# 
# to add enough Markdown cells to explain what you decided to do and to report any observations you make.
# to generate maps to visualize your neighborhoods and how they cluster together.
# Once you are happy with your analysis, submit a link to the new Notebook on your Github repository. (3 marks)
# 
# ## Explore Neighborhoods in Manhattan

# In[14]:


CLIENT_ID = 'HMJSQ3GVV5LKNSTVIMD1DZYOS1E2WPOYUF43FCPWX4QNKLFW' # your Foursquare ID
CLIENT_SECRET = 'PWDCF25ERDO4YXDNO4IINJGEKEOBCJO2P0WSGINATUEJWIGR' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version
LIMIT = 100 # limit of number of venues returned by Foursquare API


# In[15]:


def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):
        print(name)
            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Neighborhood', 
                  'Neighborhood Latitude', 
                  'Neighborhood Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    return(nearby_venues)


# #### Now write the code to run the above function on each neighborhood and create a new dataframe called *toronto_venues*.

# In[16]:


import requests # library to handle requests


# In[17]:


toronto_venues = getNearbyVenues(names=table3['Neighbourhood'],
                                   latitudes=table3['Latitude'],
                                   longitudes=table3['Longitude']
                                  )



# #### Let's check the size of the resulting dataframe

# In[18]:


print(toronto_venues.shape)
toronto_venues.head()


# Let's check how many venues were returned for each neighborhood

# In[19]:


toronto_venues.groupby('Neighborhood').count()


# #### Let's find out how many unique categories can be curated from all the returned venues

# In[20]:


print('There are {} uniques categories.'.format(len(toronto_venues['Venue Category'].unique())))


# ##  Analyze Each Neighborhood

# In[21]:


# one hot encoding
toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
toronto_onehot['Neighborhood'] = toronto_venues['Neighborhood'] 

# move neighborhood column to the first column
fixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])
toronto_onehot = toronto_onehot[fixed_columns]

toronto_onehot.head()


# And let's examine the new dataframe size.

# In[22]:


toronto_onehot.shape


# #### Next, let's group rows by neighborhood and by taking the mean of the frequency of occurrence of each category

# In[23]:


toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()
toronto_grouped


# #### Let's confirm the new size

# In[24]:


toronto_grouped.shape


# #### Let's print each neighborhood along with the top 5 most common venues

# In[25]:


num_top_venues = 5

for hood in toronto_grouped['Neighborhood']:
    print("----"+hood+"----")
    temp = toronto_grouped[toronto_grouped['Neighborhood'] == hood].T.reset_index()
    temp.columns = ['venue','freq']
    temp = temp.iloc[1:]
    temp['freq'] = temp['freq'].astype(float)
    temp = temp.round({'freq': 2})
    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))
    print('\n')


# #### Let's put that into a *pandas* dataframe
# 
# First, let's write a function to sort the venues in descending order.

# In[26]:


def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]


# Now let's create the new dataframe and display the top 10 venues for each neighborhood.

# In[27]:


num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Neighborhood']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
neighborhoods_venues_sorted = pd.DataFrame(columns=columns)
neighborhoods_venues_sorted['Neighborhood'] = toronto_grouped['Neighborhood']

for ind in np.arange(toronto_grouped.shape[0]):
    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)

neighborhoods_venues_sorted.head()


# ## 4. Cluster Neighborhoods
# 
# Run *k*-means to cluster the neighborhood into 5 clusters.

# In[28]:


# import k-means from clustering stage
from sklearn.cluster import KMeans


# In[29]:


# set number of clusters
kclusters = 5
toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)

# check cluster labels generated for each row in the dataframe
kmeans.labels_[0:10] 


# Let's create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood.

# In[30]:


# Note that the Neighborhood column in table3 is spelled as 'Neighbourhood'
table3.head(1)


# In[31]:


# add clustering labels
neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)

toronto_merged = table3

# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood
toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighbourhood',how = 'right')

toronto_merged.head() # check the last columns!


# Finally, let's visualize the resulting clusters

# In[32]:


#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab
import folium # map rendering library

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors


# In[33]:


# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighbourhood'], toronto_merged['Cluster Labels']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
       
map_clusters


# ## 5. Examine Clusters
# 
# Now, you can examine each cluster and determine the discriminating venue categories that distinguish each cluster. 

# #### Cluster 0
# #### City cluster
# 
# This cluster is indicated by red dots in the map

# In[42]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 0, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]


# #### Cluster 1
# 
# #### Park cluster
# 
# This cluster is indicated by purple dots in the map

# In[43]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 1, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]


# #### Cluster 2
# #### Cafeteria cluster
# 
# This cluster is indicated by a blue dot in the map

# In[44]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 2, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]


# #### Cluster 3
# #### Fast food cluster
# 
# This cluster is indicated by a green dot in the map

# In[45]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 3, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]


# #### Cluster 4
# #### Sports cluster
# 
# This cluster is indicated by orange dots in the map

# In[46]:


toronto_merged.loc[toronto_merged['Cluster Labels'] == 4, toronto_merged.columns[[2] + list(range(5, toronto_merged.shape[1]))]]

